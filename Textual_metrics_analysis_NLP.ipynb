{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GSP31/GSP31/blob/main/Textual_metrics_analysis_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install requests beautifulsoup4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-CHVgQq7f-D",
        "outputId": "e0f7ca92-ecce-4971-aef2-b91aaf18146d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.27.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import requests\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "from textblob import TextBlob, Word\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "%matplotlib inline\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McndBnxTnNsL",
        "outputId": "c9f3bafa-5153-4413-aa1f-a01b717cbf75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read input file\n",
        "input_file = pd.read_excel('Input.xlsx')"
      ],
      "metadata": {
        "id": "oSzkSDbXi6kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define stop words lists\n",
        "stopwords_auditor = set(open('StopWords_Auditor.txt').read().split())\n",
        "stopwords_currencies = set(open('StopWords_Currencies.txt', encoding='ISO-8859-1').read().split())\n",
        "stopwords_datesandnumbers = set(open('StopWords_DatesandNumbers.txt').read().split())\n",
        "stopwords_generic = set(open('StopWords_Generic.txt').read().split())\n",
        "stopwords_genericlong = set(open('StopWords_GenericLong.txt').read().split())\n",
        "stopwords_geographic = set(open('StopWords_Geographic.txt').read().split())\n",
        "stopwords_names = set(open('StopWords_Names.txt').read().split())\n",
        "stopwords = stopwords_auditor.union(stopwords_currencies, stopwords_datesandnumbers, stopwords_generic, stopwords_genericlong, stopwords_geographic, stopwords_names)"
      ],
      "metadata": {
        "id": "Wpxv4Mjii9Gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define positive and negative words lists\n",
        "positive_words = set(open('positive-words.txt').read().split())\n",
        "negative_words = set(open('negative-words.txt', encoding='ISO-8859-1').read().split())\n"
      ],
      "metadata": {
        "id": "BUkGVWigi-v8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define function to calculate syllable count per word\n",
        "def syllable_count(word):\n",
        "    vowels = \"aeiou\"\n",
        "    count = 0\n",
        "    if word[0] in vowels:\n",
        "        count += 1\n",
        "    for i in range(1, len(word)):\n",
        "        if word[i] in vowels and word[i-1] not in vowels:\n",
        "            count += 1\n",
        "    if word.endswith(('es', 'ed')):\n",
        "        count -= 1\n",
        "    if count == 0:\n",
        "        count += 1\n",
        "    return count"
      ],
      "metadata": {
        "id": "o809yCuth498"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loop through the URLs and compute variables\n",
        "output = []\n",
        "for index, row in input_file.iterrows():\n",
        "    url_id = row['URL_ID']\n",
        "    url = row['URL']\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    # check if the article tag is present\n",
        "    if not soup.find_all('article'):\n",
        "        print(f'Article not found for URL {url}')\n",
        "        continue\n",
        "    # extract article text\n",
        "    article_text = ''\n",
        "    for p in soup.find_all('article')[0].find_all('p'):\n",
        "        article_text += p.get_text().strip() + ' '\n",
        "    # save article text to file\n",
        "    with open(f'{url_id}.txt', 'w') as f:\n",
        "        f.write(article_text)\n",
        "    # clean text using stop words lists\n",
        "    cleaned_text = ''\n",
        "    for word in word_tokenize(article_text):\n",
        "        word = word.lower()\n",
        "        if word not in stopwords and word not in string.punctuation:\n",
        "            cleaned_text += word + ' '\n",
        "    # create dictionary of positive and negative words\n",
        "    pos_words = [w for w in word_tokenize(cleaned_text) if w in positive_words]\n",
        "    neg_words = [w for w in word_tokenize(cleaned_text) if w in negative_words]\n",
        "    # calculate positive and negative scores\n",
        "    pos_score = len(pos_words)\n",
        "    neg_score = len(neg_words)\n",
        "    # calculate polarity and subjectivity scores\n",
        "    polarity_score = (pos_score - neg_score) / ((pos_score + neg_score) + 0.000001)\n",
        "    subj_score = (pos_score + neg_score) / ((len(word_tokenize(cleaned_text))) + 0.000001)\n",
        "    # calculate average sentence length, percentage of complex words, and fog index\n",
        "    sentences = sent_tokenize(article_text)\n",
        "    num_sentences = len(sentences)\n",
        "    num_words = len(word_tokenize(cleaned_text))\n",
        "    avg_sent_len = num_words / num_sentences\n",
        "    num_complex_words = len([word for word in word_tokenize(cleaned_text) if syllable_count(word) > 2])\n",
        "    percent_complex_words = (num_complex_words / num_words)*100\n",
        "    fog_index = 0.4 * (avg_sent_len + percent_complex_words)\n",
        "    # calculate average number of words per sentence\n",
        "    avg_words_per_sent = num_words / num_sentences\n",
        "    #Although im not getting whats difference between average sentence length and average no of words per sentence it seems same to me\"\n",
        "\n",
        "    # calculate complex word count\n",
        "    complex_word_count = num_complex_words\n",
        "    # calculate word count\n",
        "    word_count = len(word_tokenize(cleaned_text))\n",
        "    # calculate syllable per word\n",
        "    syll_per_word = sum(syllable_count(word) for word in word_tokenize(cleaned_text)) / word_count\n",
        "    # calculate personal pronouns count\n",
        "    personal_pronouns = len(re.findall(r'\\b(I|we|my|ours|us)\\b', cleaned_text))\n",
        "    # calculate average word length\n",
        "    word_lengths = [len(word) for word in word_tokenize(cleaned_text)]\n",
        "    avg_word_len = sum(word_lengths) / len(word_tokenize(cleaned_text))\n",
        "    # append results to output list\n",
        "    output.append({\n",
        "        'URL_ID': url_id,\n",
        "        'URL': url,\n",
        "        'Positive Score': pos_score,\n",
        "        'Negative Score': neg_score,\n",
        "        'Polarity Score': polarity_score,\n",
        "        'Subjectivity Score': subj_score,\n",
        "        'Average Sentence Length': avg_sent_len,\n",
        "        'Percentage of Complex Words': percent_complex_words,\n",
        "        'Fog Index': fog_index,\n",
        "        'Average Number of Words per Sentence': avg_words_per_sent,\n",
        "        'Complex Word Count': complex_word_count,\n",
        "        'Word Count': word_count,\n",
        "        'Syllables per Word': syll_per_word,\n",
        "        'Personal Pronouns': personal_pronouns,\n",
        "        'Average Word Length': avg_word_len\n",
        "    })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLls8S32h_gd",
        "outputId": "aaa9271c-6d37-45ac-ca24-73ec4d4f3748"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Article not found for URL https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
            "Article not found for URL https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n",
            "Article not found for URL https://insights.blackcoffer.com/ensuring-growth-through-insurance-technology/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create output file\n",
        "output_file = pd.DataFrame(output)\n",
        "output_file.to_excel('Output.xlsx', index=False)"
      ],
      "metadata": {
        "id": "ncZeLjhzhqZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U1yO9a0b8YxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MqAFnimi8ZpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u9PcXm2D8aSj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}